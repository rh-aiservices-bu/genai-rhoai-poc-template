{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langchain langchain-community openai httpx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_url = \"your-url-here\"\n",
    "\n",
    "prompt=\"\"\"\n",
    "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction: What is sparsity? ### Response:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send a request with the requests library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are unsure what your model name is, you can go to your endpoint and add a /docs at the end to get to the Swagger UI.  \n",
    "From here you can easily send a request to show available models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "obj = {\"prompt\": prompt, \"model\":\"model_name\"}\n",
    "print(requests.post(f\"{inference_url}/v1/completions\", json=obj, verify=False).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Send a request with langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "import httpx\n",
    "\n",
    "\n",
    "llm = OpenAI(\n",
    "    openai_api_key=\"EMPTY\",\n",
    "    openai_api_base= f\"{inference_url}/v1\",\n",
    "    model_name=\"model_name\",\n",
    "    top_p=0.92,\n",
    "    temperature=0.01,\n",
    "    max_tokens=512,\n",
    "    presence_penalty=1.03,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],\n",
    "    async_client=httpx.AsyncClient(verify=False), #Skips SSL verification, remove if not needed\n",
    "    http_client=httpx.Client(verify=False) #Skips SSL verification, remove if not needed\n",
    ")\n",
    "\n",
    "llm(prompt)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
