kind: Deployment
apiVersion: apps/v1
metadata:
  name: docling-server
  labels:
    app: docling-server
spec:
  replicas: 1
  selector:
    matchLabels:
      app: docling-server
  template:
    metadata:
      name: docling-server
      labels:
        app: docling-server
    spec:
      volumes:
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: 2Gi
      containers:
        - resources:
            limits:
              cpu: '6'
              memory: 24Gi
              nvidia.com/gpu: '1'
            requests:
              cpu: '4'
              memory: 8Gi
              nvidia.com/gpu: '1'
          readinessProbe:
            tcpSocket:
              port: 8080
            timeoutSeconds: 1
            periodSeconds: 10
            successThreshold: 1
            failureThreshold: 3
          terminationMessagePath: /dev/termination-log
          name: kserve-container
          ports:
            - containerPort: 8080
              protocol: TCP
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - name: shm
              mountPath: /dev/shm
          terminationMessagePolicy: File
          image: 'quay.io/rh-aiservices-bu/docling-server:cuda-0.0.7'
      restartPolicy: Always
      terminationGracePeriodSeconds: 30
      dnsPolicy: ClusterFirst
      securityContext: {}
      schedulerName: default-scheduler
      tolerations:
        - key: nvidia.com/gpu
          operator: Equal
          value: Tesla-T4-SHARED
          effect: NoSchedule
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 25%
      maxSurge: 25%
  revisionHistoryLimit: 10
  progressDeadlineSeconds: 600
