apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  name: hf-tgi-runtime
spec:
  containers:
    - name: kserve-container
      image: ghcr.io/huggingface/text-generation-inference:1.4.0
      command: ["text-generation-launcher"]
      args:
        - "--model-id=/mnt/models/"
        - "--port=3000"
      env:
      - name: HF_HOME
        value: /tmp/hf_home
      - name: HUGGINGFACE_HUB_CACHE
        value: /tmp/hf_hub_cache
      - name: TRANSFORMER_CACHE
        value: /tmp/transformers_cache
      #resources: # configure as required
      #  requests:
      #    nvidia.com/gpu: 1
      #  limits:
      #    nvidia.com/gpu: 1
      readinessProbe: # Use exec probes instad of httpGet since the probes' port gets rewritten to the containerPort
        exec:
          command:
            - curl
            - localhost:3000/health
        initialDelaySeconds: 30
      livenessProbe:
        exec:
          command:
            - curl
            - localhost:3000/health
        initialDelaySeconds: 30
      ports:
        - containerPort: 3000
          protocol: TCP
  multiModel: false
  supportedModelFormats:
    - autoSelect: true
      name: pytorch
